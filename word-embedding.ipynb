{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Giới thiệu"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Word2vec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.1. Giới thiệu\n",
    "\n",
    "Word2vec là một kỹ thuật được giới thiệu bởi Tomas Mikolov vào năm 2013 để học word embeddings từ một lượng lớn dữ liệu văn bản. Word2vec không cần dữ liệu đã được gắn nhãn (như phân loại từ); thay vào đó, nó sử dụng self-supervised learning, nghĩa là nó tự học từ chính dữ liệu bằng cách dự đoán một phần dữ liệu dựa trên phần khác.\n",
    "\n",
    "Word2vec có hai mô hình chính:\n",
    "- **Skip-gram**: Skip-gram dự đoán các từ ngữ cảnh dựa trên từ mục tiêu. Với từ \"sits\", mô hình cố gắng dự đoán các từ như \"the\", \"cat\", \"on\", \"the\", \"mat\".\n",
    "- **Continuous Bag of Words (CBOW)**: Ngược lại với Skip-gram, dự đoán một từ mục tiêu dựa trên các từ ngữ cảnh xung quanh nó. Ví dụ, với câu \"the cat sits on the mat\", CBOW lấy các từ \"the\", \"cat\", \"on\", \"the\", \"mat\" để dự đoán từ \"sits\".\n",
    "\n",
    "Dưới đây là bảng so sánh nhanh giữa hai mô hình:\n",
    "\n",
    "|Mô hình |\tMục tiêu |\tVí dụ|\n",
    "|--------|-----------|-------|\n",
    "|Skip-gram|\tDự đoán context từ center word|\tTừ \"ăn\" dự đoán \"tôi\" và \"táo\".|\n",
    "|Continuous Bag of Words (CBOW)|\tDự đoán center word từ context|\tTừ \"tôi\" và \"táo\" dự đoán \"ăn\".|\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <div style=\"display: inline-block;\">\n",
    "        <p style=\"font-style: italic; color: gray; text-align: center; margin: 4px 0 0 0;\">Bảng 1: So sánh Skip-gram và CBOW.</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <div style=\"display: inline-block;\">\n",
    "        <img src=\"https://github.com/hcmut-aa-math/word-embedding/blob/master/assets/pic3-cbow-and-skipgram.png?raw=true\" style=\"width: 600px; height: auto;\" />\n",
    "        <p style=\"font-style: italic; color: gray; text-align: center; margin: 4px 0 0 0;\">\n",
    "            Hình 3: Kiến trúc CBOW dự đoán từ hiện tại dựa trên ngữ cảnh, còn Skip-gram dự đoán các từ xung quanh dựa vào từ hiện tại.\n",
    "        </p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "Cả hai mô hình đều dựa trên các công thức toán học để tính xác suất điều kiện, sử dụng hàm softmax và dot product giữa các vector từ. Dưới đây là phân tích chi tiết từng công thức."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.2. Skip-gram\n",
    "### 2.2.1 Định nghĩa\n",
    "Mô hình skip-gam giả định rằng một từ có thể được sử dụng để sinh ra các từ xung quanh nó trong một chuỗi văn bản. Ví dụ, giả sử chuỗi văn bản là “the”, “man”, “loves”, “his” và “son”. Ta sử dụng “loves” làm từ đích trung tâm và đặt kích thước cửa sổ ngữ cảnh bằng 2. Như mô tả trong Hình 2, với từ đích trung tâm “loves”, mô hình skip-gram quan tâm đến xác suất có điều kiện sinh ra các từ ngữ cảnh (“the”, “man”, “his” và “son”) nằm trong khoảng cách không quá 2 từ:\n",
    "\n",
    "$$\n",
    "P(\\textrm{\"the\"},\\textrm{\"man\"},\\textrm{\"his\"},\\textrm{\"son\"}\\mid\\textrm{\"loves\"}).\n",
    "$$\n",
    "\n",
    "Ta giả định rằng, với từ đích trung tâm cho trước, các từ ngữ cảnh được sinh ra độc lập với nhau. Trong trường hợp này, công thức trên có thể được viết lại thành\n",
    "\n",
    "$$\n",
    "P(\\textrm{\"the\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"man\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"his\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"son\"}\\mid\\textrm{\"loves\"}).\n",
    "$$\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <div style=\"display: inline-block;\">\n",
    "        <img src=\"https://github.com/hcmut-aa-math/word-embedding/blob/master/assets/pic4-skipgram.png?raw=true\" style=\"width: 600px; height: auto;\" />\n",
    "        <p style=\"font-style: italic; color: gray; text-align: center; margin: 4px 0 0 0;\">\n",
    "            Hình 4: Mô hình skip-gram quan tâm đến xác suất có điều kiện sinh ra các từ ngữ cảnh với một từ đích trung tâm cho trước..\n",
    "        </p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "Trong mô hình skip-gam, mỗi từ được biểu diễn bằng hai vector $d-\\text{chiều}$ (một dùng khi từ $w$ là từ ngữ cảnh, một dùng khi từ $w$ là từ trung tâm) để tính xác suất có điều kiện. Giả sử chỉ số của một từ trong từ điển là $i$, vector của từ được biểu diễn là $\\mathbf{v}_i\\in\\mathbb{R}^d$ khi từ này là từ đích trung tâm và là $\\mathbf{u}_i\\in\\mathbb{R}^d$ khi từ này là một từ ngữ cảnh. Gọi $c$ và $o$ lần lượt là chỉ số của từ đích trung tâm $w_c$ và từ ngữ cảnh $w_o$ trong từ điển. Có thể thu được xác suất có điều kiện sinh ra từ ngữ cảnh cho một từ đích trung tâm cho trước bằng phép toán softmax trên tích vô hướng của vector:\n",
    "\n",
    "$$\n",
    "P(w_o \\mid w_c) = \\frac{\\exp(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{ \\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)},\n",
    "$$\n",
    "\n",
    "Trong đó:\n",
    "- $w_c$: Từ trung tâm (center word), ví dụ \"cat\" trong câu \"the cat sat on mat\".\n",
    "- $w_o$: Từ ngữ cảnh (output word), ví dụ \"sat\" trong cùng câu.\n",
    "- $\\mathbf{v}_c \\in \\mathbb{R}^d$: Vector biểu diễn của từ trung tâm $w_c$ (input vector).\n",
    "- $\\mathbf{u}_o \\in \\mathbb{R}^d$: Vector biểu diễn của từ ngữ cảnh $w_o$ (output vector).\n",
    "- $\\mathcal{V}$: Tập hợp tất cả từ trong từ điển (vocabulary). Tập chỉ số trong bộ từ vựng là $\\mathcal{V} = \\{0, 1, \\ldots, |\\mathcal{V}|-1\\}$.\n",
    "- $\\mathbf{u}_i \\in \\mathbb{R}^d$: Vector biểu diễn của từ $i$ trong từ điển.\n",
    "- $\\exp$: Hàm mũ, $\\exp(x) = e^x$\n",
    "- $\\mathbf{u}_o^\\top \\mathbf{v}_c$: Tích vô hướng (dot product) giữa vector $\\mathbf{u}_o$ và $\\mathbf{v}_c$, đo lường mức độ tương đồng giữa từ trung tâm và từ ngữ cảnh.\n",
    "- $\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)$: Tổng chuẩn hóa (normalization term) giữa các điểm tương đồng mũ giữa từ trung tâm $w_c$ trên toàn bộ từ điển $\\mathcal{V}$, đảm bảo các xác suất cộng lại bằng 1.\n",
    "\n",
    "**Ví dụ**:\n",
    "\n",
    "Giả sử d=2 (vector 2 chiều), từ trung tâm $w_c = \\text{\"cat\"}$, từ ngữ cảnh $w_o = \\text{\"pet\"}$, và từ không liên quan $w_i = \\text{\"table\"}$.\n",
    "\n",
    "$\\mathbf{v}_{\\text{cat}} = [1, 0]$, $\\mathbf{u}_{\\text{pet}} = [0.7, 0.4]$, $\\mathbf{u}_{\\text{table}} = [0.1, -0.5]$.\n",
    "\n",
    "Tích vô hướng:\n",
    "  - $\\mathbf{u}_{\\text{pet}}^\\top \\mathbf{v}_{\\text{cat}} = 0.7 \\cdot 1 + 0.4 \\cdot 0 = 0.7$\n",
    "  - $\\mathbf{u}_{\\text{table}}^\\top \\mathbf{v}_{\\text{cat}} = 0.1 \\cdot 1 + (-0.5) \\cdot 0 = 0.1$\n",
    "\n",
    "Hàm mũ:\n",
    "  - $\\exp(0.7) \\approx 2.0138$\n",
    "  - $\\exp(0.1) \\approx 1.1052$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  }
 ]
}
