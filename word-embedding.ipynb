{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Giới thiệu\n",
    "Xử lý Ngôn ngữ Tự nhiên (NLP) là một lĩnh vực trong trí tuệ nhân tạo (AI) tập trung vào việc giúp máy tính hiểu và xử lý ngôn ngữ con người. NLP được sử dụng trong nhiều ứng dụng như dịch máy, trợ lý ảo (như Siri hoặc Alexa), hoặc phân tích cảm xúc từ các bài đánh giá trên mạng. Mục tiêu của NLP là giúp máy tính có thể đọc, hiểu và trả lời các câu hỏi bằng ngôn ngữ tự nhiên, giống như cách chúng ta giao tiếp với nhau.\n",
    "\n",
    "Tuy nhiên, ngôn ngữ con người rất phức tạp. Mỗi từ có thể có nhiều nghĩa, và ý nghĩa của một từ có thể thay đổi tùy thuộc vào ngữ cảnh. Để máy tính hiểu được ngôn ngữ, chúng ta cần một cách để biểu diễn các từ một cách ý nghĩa và hiệu quả. Đó là nơi mà word embeddings (biểu diễn từ) xuất hiện."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Đặt vấn đề: Giới hạn của biểu diễn one-hot\n",
    "\n",
    "Trong các tác vụ NLP, bước đầu tiên và quan trọng là **biểu diễn từ ngữ dưới dạng số** để đưa vào mô hình học máy. Một phương pháp trực quan và phổ biến là **vector one-hot**, trong đó mỗi từ được biểu diễn bằng một vector nhị phân có độ dài bằng kích thước từ điển, với đúng một phần tử bằng 1 và các phần tử còn lại bằng 0.\n",
    "\n",
    "Mặc dù đơn giản và dễ triển khai, biểu diễn one-hot tồn tại những **hạn chế nghiêm trọng**:\n",
    "\n",
    "- **Không có thông tin ngữ nghĩa**: Vector của “cat” và “dog” hoàn toàn trực giao, không phản ánh bất kỳ sự tương đồng nào giữa chúng.\n",
    "- **Sparsity cao**: Độ dài vector bằng kích thước từ điển (có thể lên tới hàng triệu từ), dẫn đến tiêu tốn bộ nhớ và tính toán kém hiệu quả.\n",
    "- **Không học được quan hệ ngữ cảnh**: Các vector không thể cập nhật để phản ánh mối quan hệ ngữ cảnh trong văn bản.\n",
    "\n",
    "Vì vậy, cần có một phương pháp biểu diễn từ tốt hơn, vượt qua được các hạn chế trên\n",
    "\n",
    "\n",
    "## 1.2. Giải pháp: Word Embedding\n",
    "\n",
    "**Word embedding** là kỹ thuật học biểu diễn các từ dưới dạng vector thực (dense vector) có chiều thấp (ví dụ từ 50 đến 100, thấp hơn nhiều so với số từ khả dĩ trong từ điển của bất kỳ ngôn ngữ thông dụng nào), sao cho các vector này **mang thông tin ngữ nghĩa và ngữ cảnh**.\n",
    "\n",
    "Khác với vector one-hot, các vector embedding được học từ dữ liệu, nhờ đó:\n",
    "- Các từ có ý nghĩa gần nhau sẽ có vector gần nhau.\n",
    "- Có thể thực hiện các phép toán hình học như:\n",
    "  $$\n",
    "  \\text{vec}(\\text{\"king\"}) - \\text{vec}(\\text{\"man\"}) + \\text{vec}(\\text{\"woman\"}) \\approx \\text{vec}(\\text{\"queen\"})\n",
    "  $$\n",
    "- Được dùng như vector biểu diễn từ cho các bài toán xử lý ngôn ngữ tự nhiên khác như: phân loại văn bản, sinh văn bản, dịch máy, v.v.\n",
    "\n",
    "Một trong những mô hình học embedding thành công và phổ biến nhất là **Word2Vec**, bao gồm hai biến thể chính: **Skip-Gram** và **CBOW**.\n",
    "\n",
    "\n",
    "Word embedding không chỉ cải thiện khả năng xử lý ngôn ngữ của mô hình, mà còn:\n",
    "- **Giảm đáng kể số chiều**\n",
    "- **Tăng hiệu quả học**: Vector liên tục giúp mô hình tổng quát tốt hơn.\n",
    "- **Cho phép học không giám sát**: Có thể huấn luyện embedding từ kho văn bản lớn mà không cần gán nhãn.\n",
    "\n",
    "\n",
    "## 1.3. Ứng dụng của Word Embedding\n",
    "\n",
    "Word embedding là nền tảng cốt lõi của hầu hết các mô hình NLP hiện đại. Một số ứng dụng tiêu biểu bao gồm:\n",
    "\n",
    "- **Tìm kiếm ngữ nghĩa**: Gợi ý truy vấn hoặc tài liệu dựa trên từ tương đồng.\n",
    "- **Phân loại văn bản**: Phân loại văn bản theo nhu cầu như phân loại email rác, phân loại xu hướng bình luận, ...\n",
    "- **Dịch máy**: Embedding giúp mô hình hiểu nghĩa và ngữ pháp của từ.\n",
    "- **Sinh văn bản**: Các mô hình như GPT sử dụng embedding làm lớp đầu vào\n",
    "\n",
    "\n",
    "## 1.4. Mục tiêu của chương này\n",
    "\n",
    "Trong chương này, chúng ta sẽ cùng tìm hiểu chi tiết mô hình **Skip-gram** và **CBOW**: cách xây dựng mô hình, các công thức toán học, và cách huấn luyện chúng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Giới thiệu\n",
    "\n",
    "Word2vec là một kỹ thuật được giới thiệu bởi Tomas Mikolov vào năm 2013 để học word embeddings từ một lượng lớn dữ liệu văn bản. Word2vec không cần dữ liệu đã được gắn nhãn (như phân loại từ); thay vào đó, nó sử dụng self-supervised learning, nghĩa là nó tự học từ chính dữ liệu bằng cách dự đoán một phần dữ liệu dựa trên phần khác.\n",
    "\n",
    "Word2vec có hai mô hình chính:\n",
    "- **Skip-gram**: Skip-gram dự đoán các từ ngữ cảnh dựa trên từ mục tiêu. Với từ \"sits\", mô hình cố gắng dự đoán các từ như \"the\", \"cat\", \"on\", \"the\", \"mat\".\n",
    "- **Continuous Bag of Words (CBOW)**: Ngược lại với Skip-gram, dự đoán một từ mục tiêu dựa trên các từ ngữ cảnh xung quanh nó. Ví dụ, với câu \"the cat sits on the mat\", CBOW lấy các từ \"the\", \"cat\", \"on\", \"the\", \"mat\" để dự đoán từ \"sits\".\n",
    "\n",
    "Dưới đây là bảng so sánh nhanh giữa hai mô hình:\n",
    "\n",
    "|Mô hình |\tMục tiêu |\tVí dụ|\n",
    "|--------|-----------|-------|\n",
    "|Skip-gram|\tDự đoán context từ center word|\tTừ \"ăn\" dự đoán \"tôi\" và \"táo\".|\n",
    "|Continuous Bag of Words (CBOW)|\tDự đoán center word từ context|\tTừ \"tôi\" và \"táo\" dự đoán \"ăn\".|\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <div style=\"display: inline-block;\">\n",
    "        <p style=\"font-style: italic; color: gray; text-align: center; margin: 4px 0 0 0;\">Bảng 1: So sánh Skip-gram và CBOW.</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <div style=\"display: inline-block;\">\n",
    "        <img src=\"https://github.com/hcmut-aa-math/word-embedding/blob/master/assets/pic3-cbow-and-skipgram.png?raw=true\" style=\"width: 600px; height: auto;\" />\n",
    "        <p style=\"font-style: italic; color: gray; text-align: center; margin: 4px 0 0 0;\">\n",
    "            Hình 3: Kiến trúc CBOW dự đoán từ hiện tại dựa trên ngữ cảnh, còn Skip-gram dự đoán các từ xung quanh dựa vào từ hiện tại.\n",
    "        </p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "Cả hai mô hình đều dựa trên các công thức toán học để tính xác suất điều kiện, sử dụng hàm softmax và dot product giữa các vector từ. Dưới đây là phân tích chi tiết từng công thức."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Skip-gram\n",
    "### 2.2.1 Định nghĩa\n",
    "Mô hình skip-gam giả định rằng một từ có thể được sử dụng để sinh ra các từ xung quanh nó trong một chuỗi văn bản. Ví dụ, giả sử chuỗi văn bản là “the”, “man”, “loves”, “his” và “son”. Ta sử dụng “loves” làm từ đích trung tâm và đặt kích thước cửa sổ ngữ cảnh bằng 2. Với từ đích trung tâm “loves”, mô hình skip-gram quan tâm đến xác suất có điều kiện sinh ra các từ ngữ cảnh (“the”, “man”, “his” và “son”) nằm trong khoảng cách không quá 2 từ:\n",
    "\n",
    "$$\n",
    "P(\\textrm{\"the\"},\\textrm{\"man\"},\\textrm{\"his\"},\\textrm{\"son\"}\\mid\\textrm{\"loves\"}).\n",
    "$$\n",
    "\n",
    "Ta giả định rằng, với từ đích trung tâm cho trước, các từ ngữ cảnh được sinh ra độc lập với nhau. Trong trường hợp này, công thức trên có thể được viết lại thành\n",
    "\n",
    "$$\n",
    "P(\\textrm{\"the\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"man\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"his\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"son\"}\\mid\\textrm{\"loves\"}).\n",
    "$$\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <div style=\"display: inline-block;\">\n",
    "        <img src=\"https://github.com/hcmut-aa-math/word-embedding/blob/master/assets/pic4-skipgram.png?raw=true\" style=\"width: 600px; height: auto;\" />\n",
    "        <p style=\"font-style: italic; color: gray; text-align: center; margin: 4px 0 0 0;\">\n",
    "            Hình 4: Mô hình skip-gram quan tâm đến xác suất có điều kiện sinh ra các từ ngữ cảnh với một từ đích trung tâm cho trước..\n",
    "        </p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "Trong mô hình skip-gam, mỗi từ được biểu diễn bằng hai vector $d-\\text{chiều}$ (một dùng khi từ $w$ là từ ngữ cảnh, một dùng khi từ $w$ là từ trung tâm) để tính xác suất có điều kiện. Giả sử chỉ số của một từ trong từ điển là $i$, vector của từ được biểu diễn là $\\mathbf{v}_i\\in\\mathbb{R}^d$ khi từ này là từ đích trung tâm và là $\\mathbf{u}_i\\in\\mathbb{R}^d$ khi từ này là một từ ngữ cảnh. Gọi $c$ và $o$ lần lượt là chỉ số của từ đích trung tâm $w_c$ và từ ngữ cảnh $w_o$ trong từ điển. Có thể thu được xác suất có điều kiện sinh ra từ ngữ cảnh cho một từ đích trung tâm cho trước bằng phép toán softmax trên tích vô hướng của vector:\n",
    "\n",
    "$$\n",
    "P(w_o \\mid w_c) = \\frac{\\exp(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{ \\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)},\n",
    "$$\n",
    "\n",
    "Trong đó:\n",
    "- $w_c$: Từ trung tâm (center word), ví dụ \"cat\" trong câu \"the cat sat on mat\".\n",
    "- $w_o$: Từ ngữ cảnh (output word), ví dụ \"sat\" trong cùng câu.\n",
    "- $\\mathbf{v}_c \\in \\mathbb{R}^d$: Vector biểu diễn của từ trung tâm $w_c$ (input vector).\n",
    "- $\\mathbf{u}_o \\in \\mathbb{R}^d$: Vector biểu diễn của từ ngữ cảnh $w_o$ (output vector).\n",
    "- $\\mathcal{V}$: Tập hợp tất cả từ trong từ điển (vocabulary). Tập chỉ số trong bộ từ vựng là $\\mathcal{V} = \\{0, 1, \\ldots, |\\mathcal{V}|-1\\}$.\n",
    "- $\\mathbf{u}_i \\in \\mathbb{R}^d$: Vector biểu diễn của từ $i$ trong từ điển.\n",
    "- $\\exp$: Hàm mũ, $\\exp(x) = e^x$\n",
    "- $\\mathbf{u}_o^\\top \\mathbf{v}_c$: Tích vô hướng (dot product) giữa vector $\\mathbf{u}_o$ và $\\mathbf{v}_c$, đo lường mức độ tương đồng giữa từ trung tâm và từ ngữ cảnh.\n",
    "- $\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)$: Tổng chuẩn hóa (normalization term) giữa các điểm tương đồng mũ giữa từ trung tâm $w_c$ trên toàn bộ từ điển $\\mathcal{V}$, đảm bảo các xác suất cộng lại bằng 1.\n",
    "\n",
    "**Ví dụ**:\n",
    "\n",
    "+ Dữ liệu và thiết lập\n",
    "  - Câu mẫu: \"The cat sits on the mat.\"\n",
    "  - Từ điển $ \\mathcal{V} $: Giả sử từ điển nhỏ gồm 5 từ: $ \\mathcal{V} = \\{\\text{\"the\"}, \\text{\"cat\"}, \\text{\"sits\"}, \\text{\"on\"}, \\text{\"mat\"}\\} $, $ |\\mathcal{V}| = 5 $.\n",
    "  - Chiều vector embedding: $ d = 2 $.\n",
    "  - Cửa sổ ngữ cảnh: window size = 1.\n",
    "  - Từ trung tâm $ w_c $: \"cat\".\n",
    "  - Từ ngữ cảnh $ w_o $: \"The\" (trước 1 từ) và \"sits\" (sau 1 từ).\n",
    "\n",
    "+ Vector embedding (giả định sau huấn luyện)\n",
    "  - Giả sử sau khi huấn luyện mô hình skip-gram, các vector embedding đã được học như sau:\n",
    "\n",
    "    - $ \\mathbf{v}_{\\text{cat}} = \\mathbf{v}_c = [0.5, -0.2] $\n",
    "    - $ \\mathbf{u}_{\\text{the}} = [0.3, 0.1] $\n",
    "    - $ \\mathbf{u}_{\\text{sits}} = [0.4, -0.3] $\n",
    "    - $ \\mathbf{u}_{\\text{on}} = [0.1, 0.8] $\n",
    "    - $ \\mathbf{u}_{\\text{mat}} = [-0.2, 0.5] $\n",
    "    - Giả sử $ \\mathbf{u}_{\\text{cat}} = [0.6, -0.1] $\n",
    "\n",
    "+ Tính tích vô hướng $ \\mathbf{u}_o^\\top \\mathbf{v}_c $\n",
    "\n",
    "  - Với $ w_o = \\text{\"the\"} $:\n",
    "  $ \\mathbf{u}_{\\text{the}}^\\top \\mathbf{v}_{\\text{cat}} $ = $ (0.3 \\cdot 0.5) + (0.1 \\cdot (-0.2)) = 0.15 - 0.02 = 0.13 $\n",
    "\n",
    "  - Với $ w_o = \\text{\"sits\"} $:\n",
    "  $ \\mathbf{u}_{\\text{sits}}^\\top \\mathbf{v}_{\\text{cat}} $ = $ (0.4 \\cdot 0.5) + ((-0.3) \\cdot (-0.2)) = 0.2 + 0.06 = 0.26 $\n",
    "\n",
    "  - Với $ w_o = \\text{\"on\"} $:\n",
    "  $ \\mathbf{u}_{\\text{on}}^\\top \\mathbf{v}_{\\text{cat}} $ = $ (0.1 \\cdot 0.5) + (0.8 \\cdot (-0.2)) = 0.05 - 0.16 = -0.11 $\n",
    "\n",
    "  - Với $ w_o = \\text{\"mat\"} $:\n",
    "  $ \\mathbf{u}_{\\text{mat}}^\\top \\mathbf{v}_{\\text{cat}} $ = $ ((-0.2) \\cdot 0.5) + (0.5 \\cdot (-0.2)) = -0.1 - 0.1 = -0.2 $\n",
    "\n",
    "  - Với $ w_o = \\text{\"cat\"} $ (tự so sánh với chính nó):\n",
    "  $ \\mathbf{u}_{\\text{cat}}^\\top \\mathbf{v}_{\\text{cat}} $ = $ (0.6 \\cdot 0.5) + ((-0.1) \\cdot (-0.2)) = 0.3 + 0.02 = 0.32 $\n",
    "\n",
    "+ Tính giá trị mũ $ \\exp(\\mathbf{u}_o^\\top \\mathbf{v}_c) $ : Sử dụng hàm mũ $ \\exp(x) = e^x $ (dùng $ e \\approx 2.718 $)\n",
    "\n",
    "  - $ \\exp(0.13) \\approx 1.138 $\n",
    "  - $ \\exp(0.26) \\approx 1.297 $\n",
    "  - $ \\exp(-0.11) \\approx 0.896 $\n",
    "  - $ \\exp(-0.2) \\approx 0.819 $\n",
    "  - $ \\exp(0.32) \\approx 1.377 $\n",
    "\n",
    "+ Tính mẫu số (chuẩn hóa softmax) : Mẫu số là tổng các giá trị mũ trên tất cả các từ trong $ V $\n",
    "\n",
    "  $ \\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c) $ = $ 1.138 + 1.297 + 0.896 + 0.819 + 1.377 \\approx 5.527 $\n",
    "\n",
    "+ Tính xác suất $ P(w_o | w_c) $ cho mỗi từ : Sử dụng công thức softmax\n",
    "\n",
    "  - $ P(\\text{\"the\"} | \\text{\"cat\"}) = \\frac{\\exp(0.13)}{5.527} = \\frac{1.138}{5.527} \\approx 0.206 $\n",
    "  - $ P(\\text{\"sits\"} | \\text{\"cat\"}) = \\frac{\\exp(0.26)}{5.527} = \\frac{1.297}{5.527} \\approx 0.235 $\n",
    "  - $ P(\\text{\"on\"} | \\text{\"cat\"}) = \\frac{\\exp(-0.11)}{5.527} = \\frac{0.896}{5.527} \\approx 0.162 $\n",
    "  - $ P(\\text{\"mat\"} | \\text{\"cat\"}) = \\frac{\\exp(-0.2)}{5.527} = \\frac{0.819}{5.527} \\approx 0.148 $\n",
    "  - $ P(\\text{\"cat\"} | \\text{\"cat\"}) = \\frac{\\exp(0.32)}{5.527} = \\frac{1.377}{5.527} \\approx 0.249 $\n",
    "\n",
    "+ Kiểm tra tổng xác suất\n",
    "\n",
    "  $ 0.206 + 0.235 + 0.162 + 0.148 + 0.249 \\approx 1.000 $\n",
    "\n",
    "  (Kết quả gần 1 do làm tròn số, chứng tỏ công thức softmax hoạt động đúng).\n",
    "\n",
    "+ Phân tích kết quả\n",
    "  - Từ ngữ cảnh thực tế: Trong câu \"The cat sits on the mat\", \"The\" và \"sits\" là các từ ngữ cảnh của \"cat\". Xác suất cao nhất thuộc về \"sits\" (0.235) và \"the\" (0.206), phù hợp với dữ liệu.\n",
    "  - Từ không liên quan: \"on\" (0.162) và \"mat\" (0.148) có xác suất thấp hơn, vì chúng nằm ngoài window size = 1 (cách \"cat\" 1 từ).\n",
    "  - Tự so sánh: \"cat\" có xác suất 0.249 khi so với chính nó, cho thấy mô hình cũng học được mối quan hệ tự phản xạ.\n",
    "\n",
    "+ Ý nghĩa\n",
    "  - Ví dụ này minh họa cách công thức softmax sử dụng tích vô hướng $ \\mathbf{u}_o^\\top \\mathbf{v}_c $ để đo lường mức độ tương thích giữa từ trung tâm và từ ngữ cảnh.\n",
    "  - Giá trị mũ $ \\exp(\\cdot) $ khuếch đại sự khác biệt và chuẩn hóa đảm bảo tổng xác suất bằng 1.\n",
    "  - Kết quả phản ánh đúng ngữ cảnh trong câu, cho thấy mô hình skip-gram học được mối quan hệ từ dữ liệu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Hàm hợp lý (likelihood)\n",
    "Giả sử ta có một chuỗi văn bản với độ dài $T$, trong đó từ tại vị trí $t$ được ký hiệu là $w^{(t)}$. Mô hình skip-gram giả định rằng:\n",
    "- Các từ ngữ cảnh trong cửa sổ kích thước $m$ (bao gồm $m$ từ bên trái và $m$ từ bên phải của từ trung tâm) được sinh ra **độc lập** với nhau, khi biết từ đích trung tâm.\n",
    "- Hàm hợp lý (likelihood) của mô hình là xác suất kết hợp để sinh ra tất cả các từ ngữ cảnh cho mọi từ trung tâm trong chuỗi văn bản.\n",
    "\n",
    "Hàm hợp lý được định nghĩa như sau:\n",
    "\n",
    "$$\n",
    "L = \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(w^{(t+j)} \\mid w^{(t)}),\n",
    "$$\n",
    "\n",
    "Nếu $t+j < 1$ hoặc $t+j > T$ (vị trí ngoài chuỗi văn bản), các số hạng này bị bỏ qua. Ví dụ, với $t=1$ và $m=2$, các vị trí $j=-2, -1$ sẽ không tồn tại và được bỏ qua.\n",
    "\n",
    "Ý nghĩa của hàm hợp lý:\n",
    "- Hàm $L$ biểu thị xác suất tổng quát để mô hình skip-gram sinh ra toàn bộ các cặp từ trung tâm và từ ngữ cảnh trong tập dữ liệu.\n",
    "- Mục tiêu của MLE là tìm các tham số ($\\mathbf{v}_i$, $\\mathbf{u}_i$) sao cho $L$ đạt giá trị lớn nhất, tức là mô hình dự đoán các từ ngữ cảnh chính xác nhất có thể.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Trainning\n",
    "Mục tiêu của skip-gram là cực đại hóa hàm hợp lý (Maximum Likelihood Estimation - MLE), tức là tìm các vector $\\mathbf{v}_i$ và $\\mathbf{u}_i$ để cho $L$ đạt giá trị lớn nhất. Hay mô hình có thể dự đoán chính xác các từ ngữ cảnh dựa trên từ trung tâm.\n",
    "\n",
    "Trong thực tế, vì $L$ là một tích của nhiều xác suất nhỏ, ta thường làm việc với log-likelihood để tránh vấn đề số học (numerical underflow):\n",
    "$$\n",
    "\\log L = \\sum_{t=1}^T \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P(w^{(t+j)} \\mid w^{(t)}).\n",
    "$$\n",
    "\n",
    "Hàm mất mát được định nghĩa là phủ định của log-likelihood:\n",
    "$$\n",
    "J = -\\log L = -\\sum_{t=1}^T \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P(w^{(t+j)} \\mid w^{(t)}).\n",
    "$$\n",
    "\n",
    "Hàm mất mát này là tổng của logarit xác suất có điều kiện trên tất cả các cặp từ trung tâm và từ ngữ cảnh trong tập huấn luyện. Việc giảm thiểu hàm mất mát này giúp mô hình cải thiện khả năng dự đoán.\n",
    "\n",
    "Cực đại hóa $\\log L$ tương đương với giảm thiểu $J$, để huấn luyện, ta sử dụng [**Stochastic Gradient Descent (SGD)**](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). Trong mỗi vòng lặp, ta lấy mẫu ngẫu nhiên một chuỗi con nhỏ hơn (mini-batch) từ tập dữ liệu, tính toán hàm mất mát cho chuỗi con đó, sau đó tính gradient để cập nhật các tham số mô hình.\n",
    "#### 2.2.3.1. Tính toán gradient: Điểm then chốt của huấn luyện\n",
    "\n",
    "Để cập nhật các vector $\\mathbf{v}_c$ và $\\mathbf{u}_o$, ta cần tính gradient của hàm mất mát đối với các tham số. Điểm mấu chốt là tính gradient của logarit xác suất có điều kiện $\\log P(w_o \\mid w_c)$, trong đó $w_c$ là từ đích trung tâm và $w_o$ là từ ngữ cảnh.\n",
    "\n",
    "Theo định nghĩa trong mô hình skip-gram, xác suất có điều kiện được tính như sau:\n",
    "$$\n",
    "P(w_o \\mid w_c) = \\frac{\\exp(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)},\n",
    "$$\n",
    "và logarit của nó là:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log P(w_o \\mid w_c)\n",
    "&= \\log \\frac{\\exp(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)} \\\\\n",
    "&= \\log \\exp(\\mathbf{u}_o^\\top \\mathbf{v}_c) - \\log\\left(\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)\\right) \\\\\n",
    "&= \\mathbf{u}_o^\\top \\mathbf{v}_c - \\log\\left(\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "#### 2.2.3.2. Chứng minh gradient theo $\\mathbf{v}_c$\n",
    "\n",
    "Ta cần tính đạo hàm của $\\log P(w_o \\mid w_c)$ theo $\\mathbf{v}_c$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\mathbf{v}_c}\\log P(w_o \\mid w_c) \n",
    "&= \\frac{\\partial}{\\partial \\mathbf{v}_c} \\left( \\mathbf{u}_o^\\top \\mathbf{v}_c - \\log\\left(\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)\\right) \\right) \\\\\n",
    "&= \\frac{\\partial}{\\partial \\mathbf{v}_c} \\mathbf{u}_o^\\top \\mathbf{v}_c - \\frac{\\partial}{\\partial \\mathbf{v}_c} \\log\\left(\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)\\right) \\\\ \n",
    "&= \\mathbf{u}_o - \\frac{1}{\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)} \\cdot \\sum_{j \\in \\mathcal{V}} \\exp(\\mathbf{u}_j^\\top \\mathbf{v}_c) \\mathbf{u}_j\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Nhận thấy:\n",
    "$$\n",
    "\\frac{\\exp(\\mathbf{u}_j^\\top \\mathbf{v}_c)}{\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)} = P(w_j \\mid w_c),\n",
    "$$\n",
    "nên:\n",
    "$$\n",
    "\\frac{1}{\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)} \\cdot \\sum_{j \\in \\mathcal{V}} \\exp(\\mathbf{u}_j^\\top \\mathbf{v}_c) \\mathbf{u}_j = \\sum_{j \\in \\mathcal{V}} P(w_j \\mid w_c) \\mathbf{u}_j.\n",
    "$$\n",
    "Do đó, gradient tổng quát là:\n",
    "$$\n",
    "\\frac{\\partial \\log P(w_o \\mid w_c)}{\\partial \\mathbf{v}_c} = \\mathbf{u}_o - \\sum_{j \\in \\mathcal{V}} P(w_j \\mid w_c) \\mathbf{u}_j.\n",
    "$$\n",
    "**Ý nghĩa**: Gradient này bao gồm hai phần:\n",
    "- $\\mathbf{u}_o$: Vector từ ngữ cảnh thực tế, đại diện cho từ $w_o$ mà mô hình cần dự đoán.\n",
    "- $\\sum_{j \\in \\mathcal{V}} P(w_j \\mid w_c) \\mathbf{u}_j$: Kỳ vọng của vector từ ngữ cảnh trên toàn bộ từ điển, dựa trên xác suất dự đoán.\n",
    "\n",
    "Gradient này được sử dụng để cập nhật $\\mathbf{v}_c$ trong SGD theo công thức:\n",
    "$$\n",
    "\\mathbf{v}_c \\leftarrow \\mathbf{v}_c + \\eta \\left( \\mathbf{u}_o - \\sum_{j \\in \\mathcal{V}} P(w_j \\mid w_c) \\mathbf{u}_j \\right),\n",
    "$$\n",
    "với $\\eta$ là tốc độ học (learning rate).\n",
    "#### 2.2.3.3. Kết quả sau huấn luyện\n",
    "\n",
    "Sau khi huấn luyện, với mỗi từ $w_i$ trong từ điển (có chỉ số $i$), ta thu được hai vector:\n",
    "\n",
    "- $\\mathbf{v}_i$: Vector từ đích trung tâm.\n",
    "- $\\mathbf{u}_i$: Vector từ ngữ cảnh.\n",
    "\n",
    "Trong các ứng dụng NLP (như phân loại văn bản, phân tích cảm xúc), **vector từ đích trung tâm $\\mathbf{v}_i$** thường được sử dụng làm biểu diễn chính cho từ $w_i$, vì nó được tối ưu hóa để dự đoán ngữ cảnh xung quanh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2.3. Continuous Bag of Words (CBOW)\n",
    "### 2.3.1.Giới thiệu tổng quan về CBOW\n",
    "\n",
    "CBOW là một kiến trúc học từ biểu diễn (word embedding) do nhóm Google giới thiệu trong mô hình Word2Vec.\n",
    "\n",
    "Mục tiêu của CBOW là: Dự đoán một từ trung tâm (target word) dựa trên các từ ngữ cảnh xung quanh nó\n",
    "\n",
    "### 2.3.1.1. Ký hiệu và dữ liệu đầu vào\n",
    "\n",
    "Giả sử chúng ta có một tập từ vựng $\\mathcal{V}$ với kích thước $|\\mathcal{V}|$.\n",
    "\n",
    "Mỗi từ được biểu diễn bằng một vector **one-hot** kích thước $|\\mathcal{V}|$.\n",
    "\n",
    "Nếu từ $w_i$ là từ thứ $i$ trong tập từ vựng, thì vector one-hot là:\n",
    "\n",
    "$$\n",
    "x_i = [0, 0, \\ldots, 1, \\ldots, 0]^T \\quad \\text{(vị trí thứ } i \\text{ là 1)}\n",
    "$$\n",
    "\n",
    "Giả sử:\n",
    "\n",
    "- Cửa sổ ngữ cảnh có kích thước $2c$ (nghĩa là lấy $c$ từ bên trái và $c$ từ bên phải của từ trung tâm $w_t$)  \n",
    "- Các từ ngữ cảnh được ký hiệu là $w_{t-j}$ và $w_{t+j}$ với $j = 1, 2, \\ldots, c$\n",
    "\n",
    "\n",
    "### 2.3.1.2. Cấu trúc mạng CBOW\n",
    "\n",
    "#### (i) Lớp đầu vào:\n",
    "- Gồm $2c$ vector one-hot:  \n",
    "  $x_{t-c}, \\ldots, x_{t-1}, x_{t+1}, \\ldots, x_{t+c}$\n",
    "\n",
    "#### (ii) Lớp ẩn (embedding):\n",
    "\n",
    "- Ma trận trọng số $W \\in \\mathbb{R}^{|\\mathcal{V}| \\times N}$, ánh xạ one-hot sang vector nhúng kích thước $N$\n",
    "- Với vector one-hot $x$, embedding vector tương ứng là:\n",
    "\n",
    "$$\n",
    "v = W^T x\n",
    "$$\n",
    "\n",
    "Do $x$ là one-hot, nên $v$ chính là một hàng của $W$\n",
    "\n",
    "- Trung bình các vector embedding từ ngữ cảnh:\n",
    "\n",
    "$$\n",
    "h = \\frac{1}{2c} \\sum_{\\substack{-c \\leq j \\leq c \\\\ j \\ne 0}} W^T x_{t+j}\n",
    "$$\n",
    "\n",
    "#### (iii) Lớp đầu ra:\n",
    "\n",
    "- Ma trận trọng số $W' \\in \\mathbb{R}^{N \\times |\\mathcal{V}|}$, ánh xạ từ không gian nhúng về xác suất từ vựng.\n",
    "\n",
    "- Tích vô hướng:\n",
    "\n",
    "$$\n",
    "u = W'^T h \\in \\mathbb{R}^{|\\mathcal{V}|}\n",
    "$$\n",
    "\n",
    "- Xác suất từ trung tâm là từ $w_t$:\n",
    "\n",
    "$$\n",
    "p(w_t \\mid \\text{context}) = \\frac{\\exp(u_t)}{\\sum_{i=1}^{|\\mathcal{V}|} \\exp(u_i)}\n",
    "$$\n",
    "\n",
    "### 2.3.1.3. Hàm mất mát (Loss function)\n",
    "\n",
    "Mục tiêu là tối đa hóa xác suất dự đoán đúng từ trung tâm $w_t$\n",
    "\n",
    "Hàm mất mát là hàm **cross-entropy** (âm log-likelihood):\n",
    "\n",
    "$$\n",
    "L = -\\log p(w_t \\mid \\text{context}) = -\\log \\left( \\frac{\\exp(u_t)}{\\sum_{i=1}^{|\\mathcal{V}|} \\exp(u_i)} \\right)\n",
    "= -u_t + \\log \\left( \\sum_{i=1}^{|\\mathcal{V}|} \\exp(u_i) \\right)\n",
    "$$\n",
    "\n",
    "### 2.3.1.4. Tóm tắt quy trình toán học\n",
    "\n",
    "Giả sử ngữ cảnh gồm các từ $w_{t-c}, \\ldots, w_{t-1}, w_{t+1}, \\ldots, w_{t+c}$, mỗi từ có vector one-hot $x_{t+j}$\n",
    "\n",
    "**Bước 1**: Lấy embedding:\n",
    "\n",
    "$$\n",
    "v_{t+j} = W^T x_{t+j} \\in \\mathbb{R}^N\n",
    "$$\n",
    "\n",
    "**Bước 2**: Trung bình embedding:\n",
    "\n",
    "$$\n",
    "h = \\frac{1}{2c} \\sum_{j \\ne 0} v_{t+j} \\in \\mathbb{R}^N\n",
    "$$\n",
    "\n",
    "**Bước 3**: Tính đầu ra:\n",
    "\n",
    "$$\n",
    "u = W'^T h \\in \\mathbb{R}^{|\\mathcal{V}|}\n",
    "$$\n",
    "\n",
    "**Bước 4**: Softmax:\n",
    "\n",
    "$$\n",
    "p(w_t \\mid \\text{context}) = \\frac{\\exp(u_t)}{\\sum_{i=1}^{|\\mathcal{V}|} \\exp(u_i)}\n",
    "$$\n",
    "\n",
    "**Bước 5**: Hàm mất mát:\n",
    "\n",
    "$$\n",
    "L = -\\log p(w_t \\mid \\text{context}) = -u_t + \\log \\left( \\sum_{i=1}^{|\\mathcal{V}|} \\exp(u_i) \\right)\n",
    "$$\n",
    "\n",
    "### 2.3.1.5. Ghi chú về huấn luyện\n",
    "\n",
    "- Trọng số $W$ và $W'$ được học bằng **gradient descent** hoặc **SGD**\n",
    "- Do softmax có mẫu số lớn (sum qua toàn bộ từ vựng), có thể dùng **Negative Sampling** để tăng hiệu suất tính toán.\n",
    "\n",
    "### 2.3.1.6. Ví dụ minh họa\n",
    "\n",
    "Cho câu: \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "- Từ trung tâm: \"jumps\"\n",
    "- Ngữ cảnh (với $c = 2$): \"brown\", \"fox\", \"over\", \"the\"\n",
    "\n",
    "→ CBOW học biểu diễn sao cho từ \"jumps\" có xác suất cao nhất khi biết 4 từ ngữ cảnh trên.\n",
    "### 2.3.1.7. Trực quan hóa CBOW: Từ ngữ cảnh đến từ trung tâm\n",
    "\n",
    "Giả sử ví dụ câu sau:  \n",
    "**\"The cat sits on the mat\"**\n",
    "\n",
    "Với cửa sổ ngữ cảnh c = 2, nếu từ trung tâm là “**sits**”, thì ngữ cảnh là:\n",
    "\n",
    "- Trái: “The”, “cat”  \n",
    "- Phải: “on”, “the”\n",
    "\n",
    "### 2.3.1.7.1 Sơ đồ dòng dữ liệu trong mô hình CBOW\n",
    "\n",
    "####  Tầng đầu vào (One-hot encoding)\n",
    "\n",
    "4 từ ngữ cảnh được chuyển sang vector one-hot kích thước $|V|$:\n",
    "x₁        x₂        x₃        x₄\n",
    "[0...1...0] [0...1...0] [0...1...0] [0...1...0] (Kích thước |V|)\n",
    "\n",
    "\n",
    "\n",
    "#### Nhúng từ (Embedding)\n",
    "\n",
    "Mỗi one-hot vector nhân với ma trận $W \\in \\mathbb{R}^{|V| \\times N}$ để lấy embedding:\n",
    "\n",
    "$$\n",
    "v_i = W^T x_i \\in \\mathbb{R}^N\n",
    "$$\n",
    "\n",
    "Ví dụ:\n",
    "v₁        v₂        v₃        v₄\n",
    "[0.2 ... 0.1] ... ... ...\n",
    "\n",
    "\n",
    "\n",
    "#### Trung bình các vector\n",
    "\n",
    "$$\n",
    "h = \\frac{1}{4} (v_1 + v_2 + v_3 + v_4)\n",
    "$$\n",
    "\n",
    "→ trung bình →\n",
    "h = [0.3, 0.5, ..., 0.1] ∈ ℝᴺ\n",
    "\n",
    "\n",
    "\n",
    "#### Tầng đầu ra (Dự đoán từ trung tâm)\n",
    "\n",
    "Nhân với ma trận $W' \\in \\mathbb{R}^{N \\times |V|}$:\n",
    "\n",
    "$$\n",
    "u = W'^T h \\in \\mathbb{R}^{|V|}\n",
    "$$\n",
    "\n",
    "u = [1.2, 0.5, 3.1, ..., 0.8]\n",
    "\n",
    "Softmax\n",
    "\n",
    "p(w_t | context) = xác suất trên toàn từ vựng\n",
    "\n",
    "####  Tóm tắt mô hình CBOW:\n",
    "Từ ngữ cảnh (x₁, ..., x₄)\n",
    "(embedding qua W)\n",
    "Vector nhúng (v₁, ..., v₄)\n",
    "(trung bình)\n",
    "Vector ngữ nghĩa h\n",
    " (ma trận W')\n",
    "Score u → Softmax → Xác suất từ trung tâm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.3.2. Training\n",
    "Tương tự như việc huấn luyện mô hình Skip-gram, ta sẽ cực tiểu hóa hàm mất mát, và sử dụng [**Stochastic Gradient Descent (SGD)**](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) để huấn luyện mô hình CBOW.\n",
    "\n",
    "#### 2.3.2.1 Tính toán gradient:\n",
    "Sau khi đã có được hàm hợp lý của mô hình CBOW là:\n",
    "$$\n",
    "L = \\prod_{t=1}^{T}  P(w^{(t)} \\mid  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}).\n",
    "$$\n",
    "\n",
    "Nhiệm vụ của chúng ta là đi tìm ước lượng hợp lý cực đại của mô hình CBOW bằng cách cực tiểu hóa hàm mất mát (tương tự như Skip-gram).<br><br>\n",
    "Để việc tối ưu diễn ra thuận tiện hơn, ta lấy logarith của hàm hợp lý để biến tích thành tổng:\n",
    "$$\n",
    "\\log (L) = \\sum_{t=1}^{T} \\log P\\left(w^{(t)} \\mid w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}\\right).\n",
    "$$\n",
    "\n",
    "Sau đó lấy phủ định để có được hàm mất mát:\n",
    "$$\n",
    "J = -\\log (L) = -\\sum_{t=1}^T  \\textrm{log}\\, P(w^{(t)} \\mid  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}).\n",
    "$$\n",
    "\n",
    "Lưu ý rằng, khi lấy logarith của phương trình **xác suất có điều kiện sinh ra từ đích trung tâm dựa vào các từ ngữ cảnh cho trước** ta sẽ có:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log\\,P(w_c \\mid \\mathcal{W}_o) \n",
    "&= \\log \\frac{\\exp\\left(\\mathbf{u}_c^\\top \\bar{\\mathbf{v}}_o\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp\\left(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o\\right)}\\\\\n",
    "&= \\log\\,\\left( \\exp\\left(\\mathbf{u}_c^\\top \\bar{\\mathbf{v}}_o\\right)\\right) - \\log\\,\\left(\\sum_{i \\in \\mathcal{V}} \\exp\\left(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o\\right)\\right)\\\\\n",
    "&= \\mathbf{u}_c^\\top \\bar{\\mathbf{v}}_o - \\log\\,\\left(\\sum_{i \\in \\mathcal{V}} \\exp\\left(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o\\right)\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### 2.3.2.2. Chứng minh gradient theo $\\mathbf{v}_{o_i}$\n",
    "Và để tìm gradient của hàm mất mát, ta lấy đạo hàm của phương trình trên theo từng vector ngữ cảnh $\\mathbf{v}_{o_i}\\ (i = 1, \\ldots, 2m)$:\n",
    "$$\n",
    "\\frac{\\partial \\log\\, P(w_c \\mid \\mathcal{W}_o)}{\\partial \\mathbf{v}_{o_i}} = \\frac{\\partial}{\\partial \\mathbf{v}_{o_i}} \\left( \\mathbf{u}_c^{\\top} \\bar{\\mathbf{v}}_o \\right) - \\frac{\\partial}{\\partial \\mathbf{v}_{o_i}} \\left( \\log \\left( \\sum_{j \\in \\mathcal{V}} \\exp(\\mathbf{u}_j^{\\top} \\bar{\\mathbf{v}}_o) \\right) \\right)\n",
    "$$\n",
    "\n",
    "Hãy tính hạng tử đầu tiên:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{v}_{o_i}} \\left( \\mathbf{u}_c^{\\top} \\bar{\\mathbf{v}}_o \\right) = \\frac{1}{2m} \\mathbf{u}_c^{\\top} \\frac{\\partial}{\\partial \\mathbf{v}_{o_i}} \\left(\\sum_{i=1}^{2m}\\mathbf{v}_{o_i} \\right) = \\frac{1}{2m} \\mathbf{u}_c\n",
    "$$\n",
    "\n",
    "Và hạng tử thứ hai:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{v}_{o_i}} \\left( \\log \\left( \\sum_{j \\in \\mathcal{V}} \\exp(\\mathbf{u}_j^{\\top} \\bar{\\mathbf{v}}_o) \\right) \\right)\n",
    "= \\frac{1}{\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^{\\top} \\bar{\\mathbf{v}}_o)} \\left( \\frac{\\partial}{\\partial \\mathbf{v}_{o_i}} \\left( \\sum_{j \\in \\mathcal{V}} \\exp(\\mathbf{u}_j^{\\top} \\bar{\\mathbf{v}}_o) \\right) \\right)\n",
    "$$\n",
    "\n",
    "Với:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{v}_{o_i}} \\exp(\\mathbf{u}_j^{\\top} \\bar{\\mathbf{v}}_o) = \\exp(\\mathbf{u}_j^{\\top} \\bar{\\mathbf{v}}_o) \\frac{\\partial}{\\partial \\mathbf{v}_{o_i}} \\left( \\mathbf{u}_j^{\\top} \\bar{\\mathbf{v}}_o \\right) = \\exp(\\mathbf{u}_j^{\\top} \\bar{\\mathbf{v}}_o) \\frac{1}{2m} \\mathbf{u}_j\n",
    "$$\n",
    "\n",
    "Ghép vào hạng tử thứ hai ta có:\n",
    "$$\n",
    "\\frac{1}{\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^{\\top} \\bar{\\mathbf{v}}_o)} \\left( \\sum_{j \\in \\mathcal{V}}\\exp(\\mathbf{u}_j^{\\top} \\bar{\\mathbf{v}}_o) \\frac{1}{2m} \\mathbf{u}_j \\right) = \\frac{1}{2m} \\sum_{j \\in \\mathcal{V}} \\frac{\\exp(\\mathbf{u}_j^{\\top} \\vec{v}_o)}{\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^{\\top} \\vec{v}_o)} \\mathbf{u}_j\n",
    "$$\n",
    "\n",
    "Kết hợp các hạng tử, ta có phương trình gradient tổng quát:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\log\\, P(w_c \\mid \\mathcal{W}_o)}{\\partial \\mathbf{v}_{o_i}} \n",
    "&= \\frac{1}{2m} \\mathbf{u}_c - \\frac{1}{2m} \\sum_{j \\in \\mathcal{V}} \\frac{\\exp(\\mathbf{u}_j^{\\top} \\vec{v}_o)}{\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^{\\top} \\vec{v}_o)} \\mathbf{u}_j \\\\\n",
    "&= \\frac{1}{2m} \\left( \\mathbf{u}_c - \\sum_{j \\in \\mathcal{V}} P(w_j \\, | \\, \\mathcal{W}_o) \\mathbf{u}_j \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Ý nghĩa**: Gradient này bao gồm hai phần:\n",
    "- $\\mathbf{u}_c$: Vector từ trung tâm thực tế, đại diện cho từ $w_c$ mà mô hình cần dự đoán.\n",
    "- $\\sum_{j \\in \\mathcal{V}} P(w_j \\mid \\mathcal{W}_o) \\mathbf{u}_j$: Kỳ vọng của vector từ trung tâm trên toàn bộ từ điển, dựa trên xác suất dự đoán từ đó với ngữ cảnh là các từ nằm trong cửa sổ cho trước.\n",
    "\n",
    "Gradient này được sử dụng để cập nhật $\\mathbf{v}_{o_i}$ trong SGD theo công thức:\n",
    "$$\n",
    "\\mathbf{v}_{o_i} \\leftarrow \\mathbf{v}_{o_i} + \\eta \\,. \\frac{1}{2m} \\left( \\mathbf{u}_c - \\sum_{j \\in \\mathcal{V}} P(w_j \\, | \\, \\mathcal{W}_o) \\mathbf{u}_j \\right),\n",
    "$$\n",
    "với $\\eta$ là tốc độ học (learning rate).\n",
    "\n",
    "#### 2.3.2.3. Kết quả sau huấn luyện\n",
    "\n",
    "Sau khi huấn luyện, với mỗi từ $w_i$ trong từ điển (có chỉ số $i$), ta thu được hai vector:\n",
    "\n",
    "- $\\mathbf{v}_i$: Vector từ ngữ cảnh.\n",
    "- $\\mathbf{u}_i$: Vector từ đích trung tâm.\n",
    "\n",
    "Trong các ứng dụng NLP (như phân loại văn bản, phân tích cảm xúc), **vector từ đích trung tâm $\\mathbf{u}_i$** thường được sử dụng làm biểu diễn chính cho từ $w_i$, trong mô hình CBOW nó được tối ưu hóa để phản ánh chính xác thông tin ngữ cảnh bao quanh $w_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bài tập"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Độ phức tạp tính toán của mỗi gradient là bao nhiêu? Nếu từ điển chứa một lượng lớn các từ, điều này sẽ gây ra vấn đề gì?\n",
    "### 3.1.1. Độ phức tạp tính toán của mỗi gradient\n",
    "\n",
    "Nhắc lại công thức tính gradient của hàm mất mát theo $\\mathbf{v}_c$ trong mô hình Skip-gram (15.1.8):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\textrm{log}\\, P(w_o \\mid w_c)}{\\partial \\mathbf{v}_c} = \\mathbf{u}_o - \\sum_{j \\in \\mathcal{V}} P(w_j \\mid w_c) \\mathbf{u}_j\n",
    "$$\n",
    "\n",
    "Nhắc lại công thức tính gradient của hàm mất mát theo $\\mathbf{v}_{o_i}$ trong mô hình CBOW (15.1.15):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\log\\, P(w_c \\mid \\mathcal{W}_o)}{\\partial \\mathbf{v}_{o_i}} = \\frac{1}{2m}\\left(\\mathbf{u}_c - \\sum_{j \\in \\mathcal{V}} P(w_j \\mid \\mathcal{W}_o) \\mathbf{u}_j \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "- Dễ thấy, đối với cả 2 mô hình, để tính được gradient ta phải tính xác suất có điều kiện cho mọi từ $w_j$ có trong từ điển $\\mathcal{V}$ với một từ cho trước.\n",
    "- Do đó, độ phức tạp tính toán của mỗi gradient trong cả 2 mô hình đều là $O(|\\mathcal{V}| \\cdot d)$, với $d$ là số chiều của vector embedding và $\\mathcal{V}$ là từ điển\n",
    "\n",
    "\n",
    "### 3.1.2. Vấn đề khi từ điển chứa một lượng lớn các từ\n",
    "\n",
    "- Từ độ phức tạp trên ta thấy, mỗi lần huấn luyện cho một cặp từ, ta đều phải tính xác suất cho tất cả các từ trong từ điển\n",
    "- Điều này rất tốn thời gian, dẫn đến huấn luyện rất chậm hội tụ, không khả thi với dữ liệu thực tế.\n",
    "\n",
    "**Giải pháp:** Dùng kỹ thuật Negative Sampling (xem mục 15.2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Có một số cụm từ cố định trong tiếng Anh bao gồm nhiều từ, chẳng hạn như “new york”. Bạn sẽ huấn luyện các vector từ của chúng như thế nào? Gợi ý: Xem phần 4 trong bài báo Word2vec[2].\n",
    "\n",
    "**Ý tưởng: Nhận diện và tạo cụm từ (phrases) trước khi huấn luyện Word2Vec**\n",
    "\n",
    "Trước khi huấn luyện Word2Vec, ta xác định ra các cụm từ phổ biến trong một ngữ cảnh nhưng ít phổ biến trong các ngữ cảnh khác, như: `\"new york\"`, `\"san francisco\"`... (trong khi cụm `\"this is\"` thì xuất hiện trong rất nhiều ngữ cảnh), sau đó nối chúng lại và coi chúng là một từ riêng trong từ điển khi huấn luyện embedding.\n",
    "\n",
    "**Phương pháp phát hiện cụm từ: Dựa trên thống kê đồng xuất hiện**\n",
    "Có nhiều phương pháp phát hiện cụm từ đã được nghiên cứu trước đó, nhưng chúng không nằm trong phạm vi nghiên cứu của bài báo nên nhóm tác giả chỉ sử dụng phương pháp đơn giản và đủ hiệu quả.\n",
    "\n",
    "Ta kiểm tra xem cặp từ $w_i, w_j$ có nên được nối thành một cụm hay không bằng công thức sau:\n",
    "\n",
    "$$\n",
    "\\text{score}(w_i, w_j) = \\frac{C(w_i w_j) - \\delta}{C(w_i) \\cdot C(w_j)}\n",
    "$$\n",
    "\n",
    "Trong đó:\n",
    "- $C(w_i)$, $C(w_j)$: tần suất của từng từ riêng lẻ,\n",
    "- $C(w_i w_j)$: tần suất của cụm 2 từ xuất hiện liên tiếp,\n",
    "- $\\delta$: một hằng số để tránh nối những cụm có tần suất thấp (thường là 5).\n",
    "\n",
    "Đồng thời đặt ra một ngưỡng cố định. Nếu score vượt ngưỡng này, ta xem $w_i w_j$ là cụm cố định.\n",
    "\n",
    "> **Ví dụ 1**:  \n",
    "> Nếu `\"new\"` xuất hiện 5000 lần, `\"york\"` 3000 lần, `\"new york\"` xuất hiện 2800 lần, thì:\n",
    ">\n",
    "> $\\text{score}(\\text{\"new\"}, \\text{\"york\"}) = \\frac{2800 - 5}{5000 \\cdot 3000} \\approx 0.000186$\n",
    "> \n",
    "> Nếu ngưỡng là 0.0001 → `\"new york\"` được giữ lại thành `\"new_york\"`.\n",
    "\n",
    "> **Ví dụ 2**:  \n",
    "> Nếu `\"this\"` xuất hiện 10000 lần, `\"is\"` 9500 lần, `\"this is\"` xuất hiện 5000 lần, thì:\n",
    ">\n",
    "> $\\text{score}(\\text{\"this\"}, \\text{\"is\"}) = \\frac{5000 - 5}{10000 \\cdot 9500} \\approx 0.0000525789474$\n",
    "> \n",
    "> Nếu ngưỡng là 0.0001 → `\"this is\"` không được nối lại thành cụm từ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Sử dụng mô hình skip-gam làm ví dụ để tìm hiểu về thiết kế của mô hình word2vec. Mối quan hệ giữa tích vô hướng của hai vector từ và độ tương tự cô-sin trong mô hình skip-gam là gì? Đối với một cặp từ có ngữ nghĩa gần nhau, tại sao hai vector từ này lại thường có độ tương tự cô-sin cao?\n",
    "\n",
    "### 3.3.1. Mối quan hệ giữa tích vô hướng của hai vector từ và độ tương tự cô-sin\n",
    "\n",
    "Nhắc lại công thức tính vô hướng trong trường hợp 2 vector từ:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{u}_{w_o}^\\top \\boldsymbol{v}_{w_c} = \\|\\boldsymbol{u}_{w_o}\\| \\cdot \\|\\boldsymbol{v}_{w_c}\\| \\cdot \\cos(\\theta)\n",
    "$$\n",
    "\n",
    "Trong đó:\n",
    "- $\\theta$: góc giữa hai vector.\n",
    "- $\\cos(\\theta)$: **độ tương tự cosine** giữa hai vector, có miền giá trị từ -1 (khi 2 vector ngược hướng) đến 1 (khi 2 vector cùng hướng) và bằng 0 khi 2 vector vuông góc\n",
    "\n",
    "=> **Tích vô hướng càng lớn (theo chiều dương)**, nghĩa là:\n",
    "- Hai vector càng **cùng hướng** (cosine gần 1),\n",
    "- Hoặc có **độ dài lớn** (cũng ảnh hưởng, nhưng không mang nghĩa ngữ nghĩa nhiều bằng góc).\n",
    "\n",
    "\n",
    "\n",
    "### 3.3.2. Vì sao các từ có nghĩa gần nhau thường có độ tương tự cosine cao?\n",
    "\n",
    "Nhắc lại công thức của Skip-gram: Với cặp từ $(w_c, w_o)$, ta tối đa hóa xác suất sau:\n",
    "\n",
    "$$\n",
    "P(w_o | w_c) = \\frac{\\exp(\\boldsymbol{u}_{w_o}^\\top \\boldsymbol{v}_{w_c})}{\\sum_{w \\in V} \\exp(\\boldsymbol{u}_w^\\top \\boldsymbol{v}_{w_c})}\n",
    "$$\n",
    "\n",
    "Quan sát công thức này ta thấy: Nếu hai từ hay cùng xuất hiện, tức là xác suất $P$ ở trên lớn, thì tích vô hướng $\\boldsymbol{u}_{w_o}^\\top \\boldsymbol{v}_{w_c}$ cũng lớn.\n",
    "\n",
    "Mặt khác, ta huấn luyện Skip-gram bằng cách cực đại hoá hợp lý xác suất $P$ đó. Do vậy, mô hình đã được huấn luyện sao cho làm tăng tích vô hướng của 2 vector từ tương tự nhau, từ đó làm tăng độ tương tự cosine.\n",
    "\n",
    "\n",
    "**Tóm lại:**\n",
    "- Trong mô hình Skip-Gram của Word2Vec, xác suất dự đoán từ phụ thuộc vào tích vô hướng giữa hai vector từ, mà tích vô hướng lại phụ thuộc trực tiếp vào độ tương tự cosine.\n",
    "- Khi các từ có nghĩa gần nhau, chúng thường xuất hiện trong những ngữ cảnh giống nhau, khiến các vector của chúng hướng gần nhau trong không gian → độ tương tự cosine cao."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
